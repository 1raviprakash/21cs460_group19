<!DOCTYPE html>

<html lang="en" xmlns="http://www.w3.org/1999/xhtml">
<head>
    <script async="" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-MML-AM_CHTML" type="text/javascript">
</script>
	<style>
figure {
  border: 1px #cccccc solid;
  padding: 4px;
  margin: auto;
}

figcaption {
  background-color: black;
  color: white;
  font-style: italic;
  padding: 2px;
  text-align: center;
}
		
* {
  box-sizing: border-box;
}

.column {
  float: left;
  width: 33.33%;
  padding: 5px;
}

/* Clearfix (clear floats) */
.row::after {
  content: "";
  clear: both;
  display: table;
}		
</style>
    <meta charset="utf-8" />
    <title>CS460-PROJECT GROUP 19</title>
</head>
<body>
    <h2 style="color: red">CS460-Machine Leraning-Project-Final Report</h2>
    <ul>
        <li style="color: green">Students::</li>
        <li>Haraprasad Dhal</li>
        <li>Ravi Prakash Singh</li>
        <li style="color: green">Instructor::</li>
        <li>Dr. Subhankar Mishra</li>
    
    </ul>
    <h2 style="color:  blue">
        Goals    
    </h2>

    <ul>
        <li>We studied weather data of Jaunpur District in Uttarpradesh to make predictions on it.
        </li>
        <li>Prediction of Temperature and Precipitation using Time Series Models(Midsem)</li>
        <li>Use of RNN Model in Prediction(Post Midsem)</li>
	 <li><a href="project_proposal.html">Here is the link to project proposal </a></li>   
    </ul>
                <img src="jaunpur_map.jpg" style="width:35%">    
 
	
<br/>	
<hr>	
    <h2 style="color: blue;">Time Series Methods(Midsem Work)</h2>
    We Decided to use Time series models on the weather data of  Janupur and make prediction on Temperature and Precipitation Values.
     
    <h3>Models:</h3>
    <h4>Auto Regression:</h4>
        Uses past Values to make prediction.
        \[ 
            
y_{t}=\beta_{0}+\beta_{1} y_{t-1}+\varepsilon_{t} \longrightarrow \mathrm{AR}(1)
\\
y_{t}=\beta_{0}+\beta_{1} y_{t-1}+\beta_{2} y_{t-2}+\varepsilon_{t} \longrightarrow \mathrm{AR}(2)
\\
y_{t}=\beta_{0}+\beta_{1} y_{t-1}+\beta_{2} y_{t-2}+.......+\beta_{P} y_{t-P}+\varepsilon_{t} \longrightarrow \mathrm{AR}(P)
        \]
    <h4>Moving-Average:</h4>
     Uses past Error Values to make prediction.
    \[
    y_{t}=\beta_{0}+\theta_{1} \varepsilon_{t-1}+\varepsilon_{t} \longrightarrow  \mathrm{MA}(1)\\
    y_{t}=\beta_{0}+\theta_{1} \varepsilon_{t-1}+\theta_{2} \varepsilon_{t-2}+\varepsilon_{t} \longrightarrow  \mathrm{MA}(2)\\

    y_{t}=\beta_{0}+\theta_{1} \varepsilon_{t-1}+\theta_{2} \varepsilon_{t-2}+...............+\theta_{Q} \varepsilon_{t-Q}+\varepsilon_{t} \longrightarrow  \mathrm{MA}(Q)\\

    \]
    <h4>ARMA Model:</h4>
    
    \(    \\ \quad y_{t}=B_{0}+B_{1} y_{t-1}+...+B_{P} y_{t-P}+\theta_1 \varepsilon_{t-1}+.....+\theta_Q \varepsilon_{t-Q}+\varepsilon_{t} \longrightarrow ARMA(P,Q)\\
    \)
    \(\varepsilon_{n}\)  is the error in \(y_{n}\) prediction.
    \(\beta_i\) and \(\theta_j\)  are coefficients.
    
We have used ARIMA,SARIMA,SARIMAX models in the Prediction.The Detail can be found in <a href="midsem_report.html">midsem presentaion</a>.
    
    
        <h3>Finding parameters for model:</h3>
    <p>
    We have used ACF plots, PACF plots and AIC scores to set the model function and find p,q,d,P,D,Q and s values.


We use ACF And PACF plots, which measure the correlation between current time period and previous time lags.
    
        <ul>
            <li><h4>Auto Correlation Function:</h4>
            	Measure direct and indirect effect of previous time lags on current value.Used to find order of Moving Average Model.
	
            </li>

            <li><h4>Partial Auto Correlation Function:</h4>
                Measure only direct effect of previous time lags on current value value.Used to find order of Auto Regressive Model.
                <br/>
            </li>

            <li><h4>AIC:</h4>
               We mainly used AIC score for model fitting upto ARIMA model but for SARIMAX we used the ACF and PACF plots to set the model function.AIC lets us to  test how well our model fits the data set without over-fitting it. The AIC score rewards models that achieve a high goodness-of-fit score and penalizes them if they become overly complex.
            </li>





        </ul>

    </p>
<h3>Review of Papers</h3>
<a href="midsem_report.html">Link to Midsem report.</a>
 <h3>Predictions:</h3>
Here are Plots of Temperature Prediction found from different models.<a href="midsem_report.html">Link to Midsem report.</a>
<div class="row">
  <div class="column">
    <figure>
  	<img src="Monthly Average Temp Prediction ARIMA.png" style="width:100%">
  	<figcaption>Monthly Average Temperature Prediction using ARIMA(4,0,4)}:
            Trained for 450 months and Tested for next 30 months. It has a RMSE of 2.935 for Mean temp being 25.051 and AIC = 1878.252.
	 </figcaption>
     </figure>

  </div>

 <div class="column">
    <figure>
  	<img src="Monthly Average Temp Prediction SARIMA.png" style="width:100%">
  	<figcaption>Monthly Average Temp Prediction SARIMA((2,0,2),(3,0,3,12)):
                Trained for 450 months and Tested for next 30 months.
 		It has a RMSE of 1.820 for Mean temp being 25.051 and AIC = 1456.189
 
	 </figcaption>
     </figure>
    
  </div>
</div>	 
<div class="row">	 
  <div class="column">
    <figure>
  	<img src="Monthly Average Temp Prediction SARIMAX (Surface Pressure).png" style="width:100%">
  	<figcaption>Monthly Average Temp Prediction SARIMAX (Surface Pressure)((2,0,2),(3,0,3,12))}:Trained for 450 months and Tested for next 30 months.
 		It has a RMSE of 1.75 for Mean temp being 25.051 and AIC = 1450.676
	 </figcaption>
     </figure>
    
  </div>
	 
    <div class="column">
    <figure>
  	<img src="Monthly Average Temp Prediction SARIMAX (Precipitation).png" style="width:100%">
  	<figcaption> Monthly Average Temp Prediction SARIMAX (Precipitation)((2,0,2),(3,0,3,12))
 		Trained for 450 months and Tested for next 30 months.
		 It has a RMSE of 1.658 for Mean temp being 25.051 and AIC = 1405.498.

	 </figcaption>
     </figure>
    
  </div>
	 
</div>
Time Series Based Models Worked Quite Well.Next, We wanted to try RNN Models for prediction.
<br/>
<hr>

<h2 style="color:blue">RNN Model(Post Midsem)</h2>

    <h2>Recurrent Neural Networks and why we choose it:</h2>
    <p>
     RNNs are the types of neural networks designed for capturing information from sequences or time
     series data.
    	
     <h3> Problems with ANN  </h3>
        
        <ul>
           <li>
                Variable size of input/output neurons.  
           </li>
           In timeseries we may change the number of inputs at a time to the neural network, so for 		that we have set the fixed number of neurons in a layer to a high value. But in RNNs 		this does not happen.
           
           <li>
                Too much computations.
           </li>
           In ANN we have to feed vectors representing each data point and the result (prediction)
           will also be similar type of data vectors. So it will be a too much 
           computation.           

           <li>
            	No parameter sharing
           </li>
           
          
       </ul>
  
    </p>
    
        <hr>
    
    <p> 
         
         <img src="RNN-rolled.png" style="width:10%"> <br/>   
         In the above diagram, a chunk of neural network, A, looks at some input x_t and outputs a 	   	value h_t. A loop allows information to be passed from one step of the network to the next.
         
         
         
         
          A recurrent neural network can be thought of as multiple copies of the same network, each 	passing a message to a successor. Consider what happens if we unroll the loop: <br/>
       
         
         
         <img src="RNN-unrolled.png" style="width:70%"> <br/>  
        This chain-like nature reveals that recurrent neural networks are intimately related to 		sequences and lists. They are the natural architecture of neural network to use for such data.
        
   </p>



    <hr>










    <h2>Types of RNNs:</h2>
    <p>
     RNNs are the types of neural networks designed for capturing information from sequences or time
     series data.
    	
        
        <ul>
           <li>
           	One to one
           </li>
           
           <li>
               One to many
           </li>
           <li>
           	Many to one           
           </li>

           <li>
            	Many to many
           </li>
           
          
       </ul>
       <img src="tensorflow-types-of-rnn.png" style="width:70%"> <br/> 
  
    </p>
 
      



    <hr>
    
    
    
    
    
    
    
    
    
     <h2>Problems in RNNs:</h2>
    <p>
    
    <img src="Schematic-diagram-of-backpropagation-training-algorithm-and-typical-neuron-model.jpg" style="width:70%"> <br/> 
    	
        
        <ul>
           <li>
           	Vanishing gradient
           </li>
           As numbers of hidden layers grows, gradient become very small and weights will hardly change.This will hamper the learning process.
           
           <li>
               Exploding Gradient
           </li>
           When individual derivatives are large,the final derivate will also become huge and weights would change drastically.
        
          
       </ul>
       
       
       Vanishing gradient problem is more in case of timeseries prediction. 
      
    </p>
 
 
    <hr>















    <h2>LSTM :The saver</h2>
    <p>
    Long Short Term Memory networks – usually just called “LSTMs” – are a special kind of RNN, capable of learning long-term dependencies. They were introduced by Hochreiter & Schmidhuber (1997).LSTMs are explicitly designed to avoid the long-term dependency problem.
    All recurrent neural networks have the form of a chain of repeating modules of neural network. In standard RNNs, this repeating module will have a very simple structure, such as a single tanh layer.	<br/>
    <img src="LSTM3-SimpleRNN.png" style="width:80%"> <br/>
    
    
    
    LSTMs also have this chain like structure, but the repeating module has a different structure. Instead of having a single neural network layer, there are four, interacting in a very special way.<br/>
     
     
    






        <ul>
           <li>
           	Idea Behind LSTMs
           </li>
           The key to LSTMs is the cell state, the horizontal line running through the top of the diagram. It runs straight down the entire chain, with only some minor linear interactions. It’s very easy for information to just flow along it unchanged.
           <br/>
     <img src="LSTM3-C-line.png" style="width:80%"> <br/>
           
           
           The LSTM does have the ability to remove or add information to the cell state, carefully regulated by structures called gates.Gates are a way to optionally let information through. They are composed out of a sigmoid neural net layer and a pointwise multiplication operation.
           <br/>
     <img src="LSTM3-gate.png" style="width:10%"> <br/>




           <li>
           A general LSTM structure 
           
           </li>
                <br/> <img src="5.png" style="width:60%"> <br/>
           
           
          
       </ul>
 
      
    </p>
 
 
    <hr>





    <h2>Our Prediction Model</h2>
        <h2>Monthly Average Temperature Prediction</h2>
        <ul>
           <li>
           
           <a href="MONTHLY_RNN_LSTM_DAILY_SingleLstm_HiddenDense.ipynb"><h3>Single LSTM with hidden Dense layer</h3> </a>
           	
           </li>
           Trained for 374 months and predicting for next 86 months.
           <br/>
           Blue colour is actual data, orange colur is Prediction for training data and green is the prediction for test data.
           
           <br/>
           <img src="MONTHLY_RNN_LSTM_DAILY_SingleLstm_HiddenDense.png" style="width:30%"> <br/>
           </br>
		LSTM layer with 64 neurons and hidden dense layer with 32 neurons. We fit it for 			epochs=100 with loss in mean_squared_error and adam as  optimizer.
	    </br>

           Train Score: 1.36 RMSE<br/>
           Test Score: 1.63 RMSE
           
           
           
           <li>
           	
           	<a href="MONTHLY_RNN_LSTM_DAILY_LSTM and Dense Hidden layers.ipynb"><h3>Stacked LSTM with two  hidden layers (1 dense layer and 1 LSTM layer)</h3></a>
           	
           </li>
           Trained for 374 months and predicting for next 86 months.
           <br/>
           Blue colour is actual data, orange colur is Prediction for training data and green is the prediction for test data.
           
           <br/>
           <img src="MONTHLY_RNN_LSTM_DAILY_LSTM and Dense Hidden layers.png" style="width:30%"> <br/>
           
		</br>
		Both LSTM layer with 100 neurons and hidden dense layer with 32 neurons. We fit it 			for epochs=100 with loss in mean_squared_error and adam as  optimizer.
		</br>
           Train Score: 1.12 RMSE<br/>
           Test Score: 1.47 RMSE
           
           
           
           
           <li>
           
           	<a href="MONTHLY_RNN_LSTM_DAILY_Bi-directional LSTM.ipynb"><h3>Bi-Directional LSTM</h3></a>
           	
           	
           </li>
           Trained for 374 months and predicting for next 86 months.
           <br/>
           Blue colour is actual data, orange colur is Prediction for training data and green is the prediction for test data.
           
           <br/>
           <img src="MONTHLY_RNN_LSTM_DAILY_Bi-directional LSTM.png" style="width:30%"> <br/>
           
</br>
Bidirectional LSTM layer with 100 neurons and  no hidden dense layer this time. We fit it for epochs=100 with loss in mean_squared_error and adam as  optimizer.
</br>
           Train Score: 1.20 RMSE<br/>
           Test Score: 1.44 RMSE
           
           
           
           <li>
           	<a href="MONTHLY_RNN_LSTM_ConvLSTM.ipynb"><h3>ConvLSTM</h3></a>
           	
           </li>
           
        Trained for 374 months and predicting for next 86 months.
           <br/>
           Blue colour is actual data, orange colur is Prediction for training data and green is the prediction for test data.
           
           <br/>
           <img src="MONTHLY_RNN_LSTM_ConvLSTM.png" style="width:30%"> <br/>
           </br>
 ConvLSTM2D layer with 64 neurons and   hidden dense layer with 32 neurons. We fit it for epochs=100 with loss in mean_squared_error and adam as  optimizer.
</br>


           Train Score: 1.16 RMSE<br/>
           Test Score: 1.51 RMSE

           
           
     
     
     
     
          
       </ul>
 
    <hr>
       
       
              <h2>Daily average Temperature Prediction</h2>
        <ul>
           <li>
           <a href="DAILY_RNN_LSTM_DAILY_SingleLstm_1 HiddenDense.ipynb"><h3>Single LSTM with hidden Dense layer</h3></a>
           	
           	
           </li>
           Trained for 11724 days(80 %) and predicting for next 2923 days (20%) .
           <br/>
           Blue colour is actual data, orange colur is Prediction for training data and green is the prediction for test data.
           
           <br/>
           <img src="DAILY_RNN_LSTM_DAILY_SingleLstm_HiddenDense.png" style="width:30%"> <br/>
           </br>
		LSTM layer with 64 neurons and hidden dense layer with 32 neurons. We fit it for 			epochs=100 with loss in mean_squared_error and adam as  optimizer.
	    </br>

           Train Score: 1.07 RMSE<br/>
           Test Score: 1.09 RMSE
           
           
           
           <li>
                      <a href="DAILY_RNN_LSTM_DAILY_SingleLstm_2hiddenlayer(1lstm +1 dense).ipynb"><h3>Stacked LSTM with two  hidden layers (1 dense layer and 1 LSTM layer)</h3></a>
           	
           	
           </li>
           Trained for 11724 days(80 %) and predicting for next 2923 days (20%) .
           <br/>
           Blue colour is actual data, orange colur is Prediction for training data and green is the prediction for test data.
           
           <br/>
           <img src="DAILY_RNN_LSTM_DAILY_SingleLstm_2hiddenlayer(1lstm +1 dense).png" style="width:30%"> <br/>
           
		</br>
		Both LSTM layer with 100 neurons and hidden dense layer with 32 neurons. We fit it 			for epochs=100 with loss in mean_squared_error and adam as  optimizer.
		</br>
           Train Score: 1.06 RMSE<br/>
           Test Score: 1.10 RMSE
           
           
           
           
           <li>
                      <a href="DAILY_RNN_LSTM_DAILY_Bi-directional LSTM.ipynb"><h3>Bi-Directional LSTM</h3></a>

           	
           </li>
           Trained for 11724 days(80 %) and predicting for next 2923 days (20%) .
           <br/>
           Blue colour is actual data, orange colur is Prediction for training data and green is the prediction for test data.
           
           <br/>
           <img src="DAILY_RNN_LSTM_DAILY_Bi-directional LSTM.png" style="width:30%"> <br/>
           
</br>
Bidirectional LSTM layer with 100 neurons and  no hidden dense layer this time. We fit it for epochs=100 with loss in mean_squared_error and adam as  optimizer.
</br>
           Train Score: 1.08 RMSE<br/>
           Test Score: 1.10 RMSE
           
           
           
           <li>
                      <a href="DAILY_RNN_LSTM_ConvLSTM.ipynb"><h3>ConvLSTM</h3></a>

           	
           </li>
           Trained for 11724 days(80 %) and predicting for next 2923 days (20%) .
           <br/>
           Blue colour is actual data, orange colur is Prediction for training data and green is the prediction for test data.
           
           <br/>
           <img src="DAILY_RNN_LSTM_ConvLSTM.png" style="width:30%"> <br/>
           </br>
 ConvLSTM2D layer with 64 neurons and   hidden dense layer with 32 neurons. We fit it for epochs=100 with loss in mean_squared_error and adam as  optimizer.
</br>


           Train Score: 1.08 RMSE<br/>
           Test Score: 1.10 RMSE

           
           
     
     
     
     
          
       </ul>
 

 
    <hr>


<h2>Summary of Results:</h2>

	<table>
		<tr>
			<th>Sl.No</th>
			<th>Model Type</th>
			<th>Model</th>
			<th>Test score (RMSE)</th>
		</tr>
		<tr>
			<th></th>
			<th></th>
			<th></th>
			<th></th>
		
		</tr>
		<tr>
			<th></th>
			<th></th>
			<th></th>
			<th></th>
		
		</tr>
		<tr>
			<th></th>
			<th></th>
			<th></th>
			<th></th>
		
		</tr>
		<tr>
			<th></th>
			<th></th>
			<th></th>
			<th></th>
		
		</tr>
		<tr>
			<th></th>
			<th></th>
			<th></th>
			<th></th>
		
		</tr>
		<tr>
			<th></th>
			<th></th>
			<th></th>
			<th></th>
		
		
		</tr>
		<tr>
			<th></th>
			<th></th>
			<th></th>
			<th></th>
		
		
		
		</tr>
		<tr>
		
			<th></th>
			<th></th>
			<th></th>
			<th></th>
		
		
		</tr>
		<tr>
			<th></th>
			<th></th>
			<th></th>
			<th></th>
		
		
		
		</tr>
		
	</table>	



	
	
</body>
